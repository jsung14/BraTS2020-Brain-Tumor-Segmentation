{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72512b35",
   "metadata": {},
   "source": [
    "# BraTS2020 Brain Tumor Segmentation\n",
    "\n",
    "Takes data from https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation, which is from the BraTS2020 Competition. <br><br> There are 4 goals of the project:\n",
    "1. Manual segmentation labels of tumor sub-regions,\n",
    "2. Clinical data of overall survival,\n",
    "3. Clinical evaluation of progression status,\n",
    "4. Uncertainty estimation for the predicted tumor sub-regions.\n",
    "\n",
    "This file is for goal #1: Brain tumor segmentation utilizing the MRI data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d2839",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your working directory first so the data downloads where you want\n",
    "#! kaggle datasets download awsaf49/brats20-dataset-training-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! unzip brats20-dataset-training-validation.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbab6e",
   "metadata": {},
   "source": [
    "## 2. Load/Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0f8d6-ea6e-4f15-93ec-ce7d932252fa",
   "metadata": {},
   "source": [
    "### Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import onnxruntime\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from skimage import data\n",
    "from skimage.util import montage\n",
    "import skimage.transform as skTrans\n",
    "from skimage.transform import rotate, resize\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch, Dataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric, compute_confusion_matrix_metric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    Resized,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ee4be-1bc3-4ab7-9ffd-293471b059a2",
   "metadata": {},
   "source": [
    "### Setup Environment and Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a947b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = os.path.expanduser('~/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Individual Work/Deep Learning/MONAI/Brain Tumor')\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)\n",
    "# List all directories in the given path\n",
    "folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
    "\n",
    "# Print the folders\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc695e",
   "metadata": {},
   "source": [
    "### Rename files that are incorrectly named\n",
    "The segmentation file in \"BraTS20_Training_355\" folder has an incorrect name. Before moving forward, rename it to maintain similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82156a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to 355 folder for renaming\n",
    "rename_PATH = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/\"\n",
    "print(rename_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name = rename_PATH + \"W39_1998.09.19_Segm.nii\"\n",
    "new_name = rename_PATH + \"BraTS20_Training_355_seg.nii\"\n",
    "\n",
    "try:\n",
    "    os.rename(old_name, new_name)\n",
    "    print(\"File has been re-named successfully!\")\n",
    "except:\n",
    "    print(\"File is already renamed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08081a",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "Now that the files are named correctly, we can explore the data. <br><br> Let's take a look at patient 147 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d878b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .nii file as a numpy array\n",
    "test_PATH = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "test_image_flair = nib.load(test_PATH + \"BraTS20_Training_147/BraTS20_Training_147_flair.nii\").get_fdata()\n",
    "print(\"Shape: \", test_image_flair.shape)\n",
    "print(\"Dtype: \", test_image_flair.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: \", test_image_flair.min())\n",
    "print(\"Max: \", test_image_flair.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39d0ad-69fb-4ec9-9548-2abc94ab5f60",
   "metadata": {},
   "source": [
    "#### Scale Data for Viewing\n",
    "The max is a little high for our liking, so let's scale the data, just using minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84896b4e-05f7-4640-8fcb-053e76aa4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a81cf8-b43e-4388-8cb7-0be746a2ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the image and then reshape it back to original dimensions\n",
    "# Keeps spatial structure while standardizing it\n",
    "test_image_flair = scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5c65d-6a8f-443e-8769-b908a0c5e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", test_image_flair.shape)\n",
    "print(\"Dtype: \", test_image_flair.dtype)\n",
    "print(\"Min: \", test_image_flair.min())\n",
    "print(\"Max: \", test_image_flair.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a50fc-63c1-41aa-ad02-06f35f08b9c6",
   "metadata": {},
   "source": [
    "Let's rescale the rest of the images from patient 147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254ab5a-bbd1-497e-b74d-1d7c293212a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale t1\n",
    "test_image_t1 = nib.load(test_PATH + \"BraTS20_Training_147/BraTS20_Training_147_t1.nii\").get_fdata()\n",
    "test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)\n",
    "\n",
    "# Rescale t1ce\n",
    "test_image_t1ce = nib.load(test_PATH + \"BraTS20_Training_147/BraTS20_Training_147_t1ce.nii\").get_fdata()\n",
    "test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)\n",
    "\n",
    "# Rescale t2\n",
    "test_image_t2 = nib.load(test_PATH + \"BraTS20_Training_147/BraTS20_Training_147_t2.nii\").get_fdata()\n",
    "test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)\n",
    "\n",
    "# Don't want to rescale the mask - let's explore it before doing anything to it\n",
    "# Mask (seg)\n",
    "test_image_seg = nib.load(test_PATH + \"BraTS20_Training_147/BraTS20_Training_147_seg.nii\").get_fdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b193f8-a158-49b3-b630-d073ac6a21b9",
   "metadata": {},
   "source": [
    "#### View by slice\n",
    "Now that we've rescaled, let's take a look at the images for a couple of different slices. Let's arbitrarily pick slice 95 and slice 105."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba56de-09cc-40b9-ac2d-8be9ea4ba8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 95\n",
    "\n",
    "print(\"Slice number: \", str(slice)) \n",
    "\n",
    "# Let's also check the shapes of the images t1 and mask\n",
    "# Modality shape\n",
    "print(\"Modality: \", test_image_t1.shape)\n",
    "\n",
    "# Segmentation shape\n",
    "print(\"Segmentation: \", test_image_seg.shape)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Plot T1\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(test_image_t1[:,:,slice], cmap='gray')\n",
    "plt.title('T1')\n",
    "\n",
    "# Plot T1CE\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
    "plt.title('T1CE')\n",
    "\n",
    "# Plot T2\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(test_image_t2[:,:,slice], cmap='gray')\n",
    "plt.title('T2')\n",
    "\n",
    "# Plot FLAIR\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(test_image_flair[:,:,slice], cmap='gray')\n",
    "plt.title('Flair')\n",
    "\n",
    "# Plot Mask (Seg)\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(test_image_seg[:,:,slice])\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a6d51-b4ec-402a-8393-a4a777e35efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 105\n",
    "\n",
    "print(\"Slice number: \", str(slice)) \n",
    "\n",
    "# Let's also check the shapes of the images t1 and mask\n",
    "# Modality shape\n",
    "print(\"Modality: \", test_image_t1.shape)\n",
    "\n",
    "# Segmentation shape\n",
    "print(\"Segmentation: \", test_image_seg.shape)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Plot T1\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(test_image_t1[:,:,slice], cmap='gray')\n",
    "plt.title('T1')\n",
    "\n",
    "# Plot T1CE\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
    "plt.title('T1CE')\n",
    "\n",
    "# Plot T2\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(test_image_t2[:,:,slice], cmap='gray')\n",
    "plt.title('T2')\n",
    "\n",
    "# Plot FLAIR\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(test_image_flair[:,:,slice], cmap='gray')\n",
    "plt.title('Flair')\n",
    "\n",
    "# Plot Mask (Seg)\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(test_image_seg[:,:,slice])\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac14a5-9753-4ba9-b1b3-d6d73e7d74bb",
   "metadata": {},
   "source": [
    "#### View all slices\n",
    "It's pretty easy to tell at this point that depending on the slice, the images will show different things. We can then take a look at all slices and see if each will give different information, important or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25bbfb8-ee44-4f71-92f3-b53fb00ee15d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,1,1)\n",
    "\n",
    "# Montage helps show all of the images\n",
    "plt.imshow(rotate(montage(test_image_t2[:,:,:]), 90, resize=True), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33452cb-14ff-4bff-8e4c-2b1704b049e8",
   "metadata": {},
   "source": [
    "The sides of the image are dark, which means they don't contain much useful information. Let's remove 50 slices from each side to see if all important information is contained. Can adjust number of slices if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd5507-a77c-4d27-b6ff-5baba336e23d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,1,1)\n",
    "\n",
    "# Let's remove 50 slices on each side since there's not much information from them\n",
    "plt.imshow(rotate(montage(test_image_t2[50:-50,:,:]), 90, resize=True), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524dac2-1f13-4388-9809-c50450f77e71",
   "metadata": {},
   "source": [
    "#### Look at different viewpoints\n",
    "Next, let's take a look at the different views of a single slice. We'll look at the transverse view (above), the frontal view, and the saggital view (side). <br> <br>\n",
    "We do this because the different viewpoints can give us different understandings of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338e506-649e-46d9-b985-d374a52c4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 95\n",
    "\n",
    "print(\"Slice: \", str(slice))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Transverse view\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_image_t2[:,:,slice], cmap='gray')\n",
    "plt.title(\"Transverse View\")\n",
    "\n",
    "# Frontal view\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(rotate(test_image_t2[:,slice,:], 90, resize=True), cmap='gray')\n",
    "plt.title(\"Frontal View\")\n",
    "\n",
    "# Saggital view\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(rotate(test_image_t2[slice,:,:], 90, resize=True), cmap='gray')\n",
    "plt.title(\"Saggital View\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed1dc7-9edd-423b-b21c-c8ec72fa9ae9",
   "metadata": {},
   "source": [
    "#### Segmented images\n",
    "Now that we've viewed much of the data, let's focus on the segmented images.<br><br>\n",
    "There are 4 important classes in the segmented images:\n",
    "1. Not Tumor (class 0)\n",
    "2. Non-Enhancing Tumor (class 1)\n",
    "3. Edema (class 2)\n",
    "4. Enhancing Tumor (class 4)<br>\n",
    "\n",
    "We'll now take a look at each of them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43479715-3f55-43a0-80ea-912f9b68ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 95\n",
    "print(\"Slice: \", str(slice))\n",
    "\n",
    "# Lets define the classes and the coloring scheme\n",
    "cmap = matplotlib.colors.ListedColormap(['#500050', '#0096FF', '#00FF00', '#CCCC00'])\n",
    "norm = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)\n",
    "\n",
    "# Not Tumor (class 0)\n",
    "seg_0 = test_image_seg.copy()\n",
    "seg_0[seg_0 != 0] = np.nan\n",
    "\n",
    "# Non-Enhancing Tumor (class 1)\n",
    "seg_1 = test_image_seg.copy()\n",
    "seg_1[seg_1 != 1] = np.nan\n",
    "\n",
    "# Edema (class 2)\n",
    "seg_2 = test_image_seg.copy()\n",
    "seg_2[seg_2 != 2] = np.nan\n",
    "\n",
    "# Enhancing Tumor (class 4)\n",
    "seg_4 = test_image_seg.copy()\n",
    "seg_4[seg_4 != 4] = np.nan\n",
    "\n",
    "# Define class names for legend\n",
    "class_names = ['class 0', 'class 1', 'class 2', 'class 4']\n",
    "legend = [plt.Rectangle((0,0),1,1, color = cmap(i), label = class_names[i]) for i in range(len(class_names))] \n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(test_image_seg[:,:,slice], cmap=cmap, norm=norm)\n",
    "plt.title('Full Mask')\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(seg_0[:,:,slice], cmap=cmap, norm=norm)\n",
    "plt.title('Seg 0')\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(seg_1[:,:,slice], cmap=cmap, norm=norm)\n",
    "plt.title('Seg 1')\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(seg_2[:,:,slice], cmap=cmap, norm=norm)\n",
    "plt.title('Seg 2')\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(seg_4[:,:,slice], cmap=cmap, norm=norm)\n",
    "plt.title('Seg 4')\n",
    "plt.legend(handles=legend, loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd6a78",
   "metadata": {},
   "source": [
    "## 3. Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your training data directory\n",
    "train_data_dir = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "\n",
    "# Get all patient directories in the training set\n",
    "train_dirs = sorted(glob.glob(os.path.join(train_data_dir, \"BraTS20_Training_*\")))\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary contains paths to the modalities and segmentation\n",
    "train_and_test_files = []\n",
    "for train_dir in train_dirs:\n",
    "    data_dict = {\n",
    "        \"t1ce\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_t1ce.nii\"),\n",
    "        \"flair\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_flair.nii\"),\n",
    "        \"seg\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_seg.nii\"),\n",
    "    }\n",
    "    train_and_test_files.append(data_dict)\n",
    "\n",
    "# Print the number of training patients\n",
    "print(f\"Total number of patients: {len(train_and_test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae66e86",
   "metadata": {},
   "source": [
    "Since we don't have a defined test set, we can split the validation set randomly to get a train/val/test split to about 65/20/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef292757-ef08-4cd3-a8ab-c8c97e6ee9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train/test and validation (20% validation)\n",
    "train_test_files, val_files = train_test_split(train_and_test_files,test_size=0.2)\n",
    "\n",
    "# Split the train/test into train (68% train) and test (12% test)\n",
    "train_files, test_files = train_test_split(train_test_files,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d78e2-9fea-40ea-b374-8b7926214df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data distribution (Train: 68%, Test: 12%, Val: 20%)\n",
    "print(f\"Train length: {len(train_files)}\")\n",
    "print(f\"Validation length: {len(val_files)}\")\n",
    "print(f\"Test length: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa1367-f1ae-47f7-b7c4-4f035328c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"Train\",\"Valid\",\"Test\"],\n",
    "        [len(train_files), len(val_files), len(test_files)],\n",
    "        align='center',\n",
    "        color=[ 'green','red', 'blue'],\n",
    "        label=[\"Train\", \"Valid\", \"Test\"]\n",
    "       )\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Data Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c2f5b-60ef-4c8c-9cc6-1497e65aecfc",
   "metadata": {},
   "source": [
    "## 4. Define Transformations for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_size = (240, 240, 160)  # Ensure a common size for spatial dimensions\n",
    "\n",
    "class ReassignClasses:\n",
    "    def __call__(self, data):\n",
    "        seg = data['seg']\n",
    "        seg[seg == 4] = 3  # Reassign class 4 to class 3\n",
    "        data['seg'] = seg\n",
    "        return data\n",
    "\n",
    "# Define transformations for training, validation, and test datasets\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"t1ce\",\"flair\", \"seg\"]),  # Load images from file paths\n",
    "        EnsureChannelFirstd(keys=[\"t1ce\",\"flair\", \"seg\"]),  # Ensure channels are first\n",
    "        ScaleIntensityd(keys=[\"t1ce\",\"flair\"]),  # Normalize intensity values\n",
    "        Resized(keys=[\"t1ce\", \"flair\", \"seg\"], spatial_size=common_size), # Ensure common size\n",
    "        RandFlipd(keys=[\"t1ce\",\"flair\", \"seg\"], spatial_axis=0, prob=0.5),  # Random flip\n",
    "        RandRotate90d(keys=[\"t1ce\",\"flair\", \"seg\"], prob=0.5),  # Random 90 degree rotation\n",
    "        RandZoomd(keys=[\"t1ce\",\"flair\", \"seg\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),  # Random zoom\n",
    "        ReassignClasses(),\n",
    "        AsDiscreted(keys=[\"seg\"], to_onehot=4),\n",
    "        EnsureTyped(keys=[\"t1ce\",\"flair\", \"seg\"]) # Ensure tensors\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Load images from file paths\n",
    "        EnsureChannelFirstd(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Ensure channels are first\n",
    "        ScaleIntensityd(keys=[\"t1ce\", \"flair\"]),  # Normalize intensity values\n",
    "        Resized(keys=[\"t1ce\", \"flair\", \"seg\"], spatial_size=common_size),  # Resize images\n",
    "        ReassignClasses(),  # Reassign segmentation labels\n",
    "        EnsureTyped(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Ensure tensors\n",
    "        AsDiscreted(keys=[\"seg\"], to_onehot=4)  # One-hot encoding for segmentation\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dd970",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders for split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MONAI datasets for training, new validation, and test sets\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "test_ds = Dataset(data=test_files, transform=val_transforms)  # Test set with no augmentation\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, num_workers=0)\n",
    "\n",
    "# Print data loader sizes\n",
    "print(f\"Training DataLoader size: {len(train_loader)}\")\n",
    "print(f\"Validation DataLoader size: {len(val_loader)}\")\n",
    "print(f\"Test DataLoader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd444a8-ee11-40b9-8f90-000c2ec22d46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_data in train_loader:\n",
    "    print(batch_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb093bab-144a-4ec5-b6c4-83e56eb1b33e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_data in train_loader:\n",
    "    inputs = batch_data[\"t1ce\"]\n",
    "    labels = batch_data[\"seg\"]\n",
    "    print(f\"Input size: {inputs.shape}, Label size: {labels.shape}\")\n",
    "    if (inputs.shape != labels.shape): \n",
    "        print(\"Mismatch on t1ce\")\n",
    "        break\n",
    "    inputs2 = batch_data[\"flair\"]\n",
    "    print(f\"Input size: {inputs2.shape}, Label size: {labels.shape}\")\n",
    "    if (inputs2.shape != labels.shape): \n",
    "        print(\"Mismatch on flair\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96a0d7-076e-4354-b7d2-1c9ea7b8a9d7",
   "metadata": {},
   "source": [
    "## 6. Create model, loss function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d485d4-3d6b-4fac-80af-23fca87025c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 4\n",
    "val_interval = 1\n",
    "VAL_AMP = False\n",
    "\n",
    "# Define the model\n",
    "device = torch.device(\"cpu\")\n",
    "model = SegResNet(\n",
    "    blocks_down = [1, 2, 2, 4],\n",
    "    blocks_up = [1, 1, 1],\n",
    "    init_filters = 8,\n",
    "    in_channels = 2,\n",
    "    out_channels = 4,\n",
    "    dropout_prob = 0.2,\n",
    ").to(device)\n",
    "\n",
    "# Define loss function, optimizer, learning rate scheduler\n",
    "loss_function = DiceLoss(\n",
    "    smooth_nr = 0,\n",
    "    smooth_dr = 1e-5,\n",
    "    squared_pred = True,\n",
    "    to_onehot_y = False,\n",
    "    sigmoid = True\n",
    ")\n",
    "# SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
    "# Step LR\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "# Metrics and post-processing\n",
    "dice_metric = DiceMetric(include_background = True, reduction = \"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background = True, reduction = \"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "# Define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "    return _compute(input)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78851b2-ba62-47e1-adc8-460d66fcb454",
   "metadata": {},
   "source": [
    "## 7. Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e0c67-7e44-4421-9706-3d5566b86435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metric_epoch_time = [[], [], []]\n",
    "epoch_loss_val = []\n",
    "metric_val = []\n",
    "metric_val_tc = []\n",
    "metric_val_wt = []\n",
    "metric_val_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "\n",
    "    model.train() # start training the model\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "\n",
    "        inputs = torch.cat([ \n",
    "                        batch_data[\"t1ce\"].to(device), \n",
    "                        batch_data[\"flair\"].to(device)], dim=1)  # Concatenating along the channel axis\n",
    "        labels = batch_data[\"seg\"].to(device)  # Segmentation labels\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step() # Update parameters\n",
    "\n",
    "        # Accumulate total epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Update training progress\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "\n",
    "     # Update learning rate scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Calc/store average loss for epoch\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_val.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval() # Evaluation mode\n",
    "        with torch.no_grad(): # Don't want gradients when validating\n",
    "            for val_data in val_loader:\n",
    "                val_inputs = torch.cat([\n",
    "                    val_data[\"t1ce\"].to(device),\n",
    "                    val_data[\"flair\"].to(device)], dim=1)  # Concatenating along the channel axis\n",
    "                val_labels = val_data[\"seg\"].to(device)  # Segmentation labels\n",
    "\n",
    "                # Make prediction\n",
    "                val_outputs = inference(val_inputs)\n",
    "\n",
    "                # Post-process outputs\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "\n",
    "                # Compute Dice metric for each batch\n",
    "                dice_metric(y_pred = val_outputs, y = val_labels)\n",
    "                dice_metric_batch(y_pred = val_outputs, y = val_labels)\n",
    "\n",
    "            # Aggregate/store validation metrics\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_val.append(metric)\n",
    "\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_val_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_val_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_val_et.append(metric_et)\n",
    "            \n",
    "            # Reset metrics for next epoch\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "            \n",
    "            # Save model if current metric is the best\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metric_epoch_time[0].append(best_metric)\n",
    "                best_metric_epoch_time[1].append(best_metric_epoch)\n",
    "                best_metric_epoch_time[2].append(time.time() - total_start)\n",
    "                \n",
    "                # Save model\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\")\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            # Print best metrics\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "   \n",
    "    # Time spent for each epoch\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "\n",
    "\n",
    "# Save the lists as a pickle file\n",
    "with open(os.path.join(root_dir, \"training_metrics.pkl\"), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'epoch_loss_val': epoch_loss_val,\n",
    "        'metric_val': metric_val,\n",
    "        'metric_val_tc': metric_val_tc,\n",
    "        'metric_val_wt': metric_val_wt,\n",
    "        'metric_val_et': metric_val_et,\n",
    "    }, f)\n",
    "\n",
    "print(\"Training metrics saved!\")\n",
    "\n",
    "\n",
    "# Calc total training time\n",
    "total_time = time.time() - total_start\n",
    "print(f\"Total time: {total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1310b21-3806-4694-a912-182403fe1c14",
   "metadata": {},
   "source": [
    "#### Plot Loss and Metric from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45729f-a01d-46e4-a0ff-8104a2fb2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved metrics\n",
    "with open(os.path.join(root_dir, \"training_metrics.pkl\"), 'rb') as f:\n",
    "    metrics = pickle.load(f)\n",
    "\n",
    "# Now you can access the lists\n",
    "epoch_loss_val = metrics['epoch_loss_val']\n",
    "metric_val = metrics['metric_val']\n",
    "metric_val_tc = metrics['metric_val_tc']\n",
    "metric_val_wt = metrics['metric_val_wt']\n",
    "metric_val_et = metrics['metric_val_et']\n",
    "best_metric_epoch_time = metrics['best_metric_epoch_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639ae67-7f06-4eb4-893f-7d6fff174538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variables for training evaluation\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot([i + 1 for i in len(epoch_loss_val)], epoch_loss_val, color='blue')\n",
    "plt.title('Epoch Loss Values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_interval * [i + 1 for i in len(metric_val)], metric_val, color='red')\n",
    "plt.title('Validation Mean Dice')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Value')\n",
    "\n",
    "plt.figure(figsize=(18,12))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(val_interval * [i + 1 for i in len(metric_val_tc)], metric_val_tc, color='red')\n",
    "plt.title('Validation Mean Dice TC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice TC Value')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(val_interval * [i + 1 for i in len(metric_val_wt)], metric_val_wt, color='red')\n",
    "plt.title('Validation Mean Dice WT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice WT Value')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(val_interval * [i + 1 for i in len(metric_val_et)], metric_val_et, color='red')\n",
    "plt.title('Validation Mean Dice ET')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice ET Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631d253-7111-4f80-94cd-2d1a9654aa05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Debugging training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887afc6c-6f7e-4125-8028-f0c73d5759ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use a smaller subset of data for debugging\n",
    "small_train_files = train_files[:10]  # Just take the first 10 patients\n",
    "small_val_files = val_files[:5]  # Just take 5 for validation\n",
    "small_test_files = test_files\n",
    "\n",
    "# Create DataLoaders for the smaller datasets\n",
    "small_train_loader = DataLoader(Dataset(data=small_train_files, transform=train_transforms), batch_size=1, shuffle=True, num_workers=0)\n",
    "small_val_loader = DataLoader(Dataset(data=small_val_files, transform=val_transforms), batch_size=1, shuffle=False, num_workers=0)\n",
    "small_test_loader = DataLoader(Dataset(data=small_test_files, transform=val_transforms), batch_size=1, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e9a12-5e41-40b6-9261-1ce784a7095a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ReassignClasses:\n",
    "    def __call__(self, data):\n",
    "        seg = data['seg']\n",
    "        #print(f\"Seg before reassignment: {seg.shape} -- {type(seg)}\")  # Debug print\n",
    "        seg[seg == 4] = 3  # Reassign class 4 to class 3\n",
    "        data['seg'] = seg\n",
    "        #print(f\"Seg after reassignment: {seg.shape} -- {type(seg)}\")  # Debug print\n",
    "        return data\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Load images from file paths\n",
    "        EnsureChannelFirstd(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Ensure channels are first\n",
    "        ScaleIntensityd(keys=[\"t1ce\", \"flair\"]),  # Normalize intensity values\n",
    "        Resized(keys=[\"t1ce\", \"flair\", \"seg\"], spatial_size=common_size),  # Resize images\n",
    "        ReassignClasses(),  # Reassign segmentation labels\n",
    "        EnsureTyped(keys=[\"t1ce\", \"flair\", \"seg\"]),  # Ensure tensors\n",
    "        AsDiscreted(keys=[\"seg\"], to_onehot=4)  # One-hot encoding for segmentation\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd01fc0-4929-4ef0-a93b-4353588b194a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for val_data in small_val_loader:\n",
    "    print(val_data)  # Print the data dictionary to inspect what is being passed\n",
    "    val_inputs, val_labels = (\n",
    "        val_data[\"t1ce\"].to(device),\n",
    "        val_data[\"seg\"].to(device)\n",
    "    )\n",
    "    print(f\"Validation Inputs: {val_inputs.shape}\")\n",
    "    print(f\"Validation Labels: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c25163-7bbf-4d57-9cc6-15dc026b55f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for val_data in small_val_loader:\n",
    "        val_inputs = torch.cat([\n",
    "            val_data[\"t1ce\"].to(device),\n",
    "            val_data[\"flair\"].to(device)], dim=1)  # Concatenating along the channel axis\n",
    "        val_labels = val_data[\"seg\"].to(device)  # Segmentation labels\n",
    "\n",
    "        # Forward pass\n",
    "        val_outputs = model(val_inputs)\n",
    "\n",
    "        # Print shapes\n",
    "        print(f\"Model Output Shape: {val_outputs.shape}\")\n",
    "        print(f\"Validation Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "        #break  # Check only one batch for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11da5f-02a7-423a-8418-c589405366d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for batch_data in small_train_loader:\n",
    "    inputs = torch.cat([ \n",
    "                        batch_data[\"t1ce\"].to(device), \n",
    "                        batch_data[\"flair\"].to(device)], dim=1)\n",
    "    labels = batch_data[\"seg\"].to(device)\n",
    "    \n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    #break  # Only run the first batch for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a0b03-4b89-4999-a96d-e89d9cd9aa71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metric_epoch_time = [[], [], []]\n",
    "epoch_loss_val = []\n",
    "metric_val = []\n",
    "metric_val_tc = []\n",
    "metric_val_wt = []\n",
    "metric_val_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "\n",
    "    model.train() # start training the model\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for batch_data in small_train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "\n",
    "        inputs = torch.cat([ \n",
    "                        batch_data[\"t1ce\"].to(device), \n",
    "                        batch_data[\"flair\"].to(device)], dim=1)  # Concatenating along the channel axis\n",
    "        labels = batch_data[\"seg\"].to(device)  # Segmentation labels\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step() # Update parameters\n",
    "\n",
    "        # Accumulate total epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Update training progress\n",
    "        print(\n",
    "            f\"{step}/{10 // small_train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "\n",
    "     # Update learning rate scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Calc/store average loss for epoch\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_val.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval() # Evaluation mode\n",
    "        with torch.no_grad(): # Don't want gradients when validating\n",
    "            for val_data in small_val_loader:\n",
    "                val_inputs = torch.cat([\n",
    "                    val_data[\"t1ce\"].to(device),\n",
    "                    val_data[\"flair\"].to(device)], dim=1)  # Concatenating along the channel axis\n",
    "                val_labels = val_data[\"seg\"].to(device)  # Segmentation labels\n",
    "\n",
    "                # Make prediction\n",
    "                val_outputs = inference(val_inputs)\n",
    "\n",
    "                # Post-process outputs\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "\n",
    "                # Compute Dice metric for each batch\n",
    "                dice_metric(y_pred = val_outputs, y = val_labels)\n",
    "                dice_metric_batch(y_pred = val_outputs, y = val_labels)\n",
    "\n",
    "            # Aggregate/store validation metrics\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_val.append(metric)\n",
    "\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_val_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_val_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_val_et.append(metric_et)\n",
    "            \n",
    "            # Reset metrics for next epoch\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "            \n",
    "            # Save model if current metric is the best\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metric_epoch_time[0].append(best_metric)\n",
    "                best_metric_epoch_time[1].append(best_metric_epoch)\n",
    "                best_metric_epoch_time[2].append(time.time() - total_start)\n",
    "                \n",
    "                # Save model\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\")\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "\n",
    "            # Print best metrics\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "   \n",
    "    # Time spent for each epoch\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "\n",
    "\n",
    "# Calc total training time\n",
    "total_time = time.time() - total_start\n",
    "print(f\"Total time: {total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7aac5c-dd3a-49f1-a1cf-0a1fdf41e2fd",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46749f67-5ff6-4d16-9a07-6cac95d46ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(root_dir + \"/best_metric_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca006db-8306-46eb-94dc-d5c0f59b7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for evaluation\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "\n",
    "# Post-processing: apply activation and thresholding for segmentation\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "dice_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, test_data in enumerate(test_loader):\n",
    "        # Prepare test inputs by concatenating T1CE and FLAIR along the channel axis\n",
    "        test_inputs = torch.cat([\n",
    "            test_data['t1ce'].to(device),\n",
    "            test_data['flair'].to(device)], dim=1) \n",
    "        \n",
    "        test_labels = test_data['seg'].to(device)  # Get segmentation labels\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = model(test_inputs)\n",
    "\n",
    "        # Post-processing outputs\n",
    "        outputs = [post_trans(i) for i in decollate_batch(outputs)]\n",
    "\n",
    "        # Compute Dice metric for this batch\n",
    "        dice_metric(y_pred=outputs, y=test_labels)\n",
    "        \n",
    "        # Get the Dice score for the batch and append to dice_scores list\n",
    "        dice_score_batch = dice_metric.aggregate().mean().item()\n",
    "        dice_scores.append(dice_score_batch)\n",
    "\n",
    "        # Reset the dice metric for the next batch\n",
    "        dice_metric.reset()\n",
    "\n",
    "        print(f\"Dice score for batch {idx + 1}: {dice_score_batch:.4f}\")\n",
    "\n",
    "# Calculate the final average Dice score across all batches\n",
    "final_dice_score = sum(dice_scores) / len(dice_scores) if dice_scores else 0.0\n",
    "print(f\"Final Dice score on test set: {final_dice_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaae8aa-f942-44e7-a792-f32b87cd1e85",
   "metadata": {},
   "source": [
    "### Visualize the model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f458a5a2-893a-4748-8546-7b2d7d27078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seg(image_seg, slice):\n",
    "    # Lets define the classes and the coloring scheme\n",
    "    cmap = matplotlib.colors.ListedColormap(['#500050', '#0096FF', '#00FF00', '#CCCC00'])\n",
    "    norm = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)\n",
    "\n",
    "    image_seg = image_seg.cpu().numpy()  # Convert to NumPy for visualization\n",
    "\n",
    "    # Not Tumor (class 0)\n",
    "    seg_0 = image_seg.copy()\n",
    "    seg_0[seg_0 != 0] = np.nan\n",
    "    \n",
    "    # Non-Enhancing Tumor (class 1)\n",
    "    seg_1 = image_seg.copy()\n",
    "    seg_1[seg_1 != 1] = np.nan\n",
    "    \n",
    "    # Edema (class 2)\n",
    "    seg_2 = image_seg.copy()\n",
    "    seg_2[seg_2 != 2] = np.nan\n",
    "    \n",
    "    # Enhancing Tumor (class 4)\n",
    "    seg_4 = image_seg.copy()\n",
    "    seg_4[seg_4 != 4] = np.nan\n",
    "    \n",
    "    # Define class names for legend\n",
    "    class_names = ['class 0', 'class 1', 'class 2', 'class 4']\n",
    "    legend = [plt.Rectangle((0,0),1,1, color = cmap(i), label = class_names[i]) for i in range(len(class_names))] \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    \n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow(image_seg[:,:,slice], cmap=cmap, norm=norm)\n",
    "    plt.title('Full Mask')\n",
    "    plt.legend(handles=legend, loc='lower left')\n",
    "    \n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow(seg_0[:,:,slice], cmap=cmap, norm=norm)\n",
    "    plt.title('Seg 0')\n",
    "    plt.legend(handles=legend, loc='lower left')\n",
    "    \n",
    "    plt.subplot(1,5,3)\n",
    "    plt.imshow(seg_1[:,:,slice], cmap=cmap, norm=norm)\n",
    "    plt.title('Seg 1')\n",
    "    plt.legend(handles=legend, loc='lower left')\n",
    "    \n",
    "    plt.subplot(1,5,4)\n",
    "    plt.imshow(seg_2[:,:,slice], cmap=cmap, norm=norm)\n",
    "    plt.title('Seg 2')\n",
    "    plt.legend(handles=legend, loc='lower left')\n",
    "    \n",
    "    plt.subplot(1,5,5)\n",
    "    plt.imshow(seg_4[:,:,slice], cmap=cmap, norm=norm)\n",
    "    plt.title('Seg 4')\n",
    "    plt.legend(handles=legend, loc='lower left')\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "111c7a37-1cdd-4c61-9a32-d20ef5e9864e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_segmentation_results(inputs, labels, outputs, idx):\n",
    "    # Display an image slice with the prediction and ground truth\n",
    "    slice_idx = inputs.shape[4] // 2  # Use middle slice for visualization\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    cmap = matplotlib.colors.ListedColormap(['#500050', '#0096FF', '#00FF00', '#CCCC00'])\n",
    "    \n",
    "    # Plot original T1CE image\n",
    "    ax[0].imshow(inputs[0, 0, :, :, slice_idx].cpu(), cmap='gray')\n",
    "    ax[0].set_title(f'T1CE - Input {idx+1}')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    ax[1].imshow(labels[0, 0, :, :, slice_idx].cpu(), cmap=cmap)\n",
    "    ax[1].set_title(f'Ground Truth - Label {idx+1}')\n",
    "    \n",
    "    # Plot model prediction\n",
    "    ax[2].imshow(outputs[0][0, :, :, slice_idx].cpu(), cmap=cmap)\n",
    "    ax[2].set_title(f'Prediction - Output {idx+1}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1af860b-eded-4820-8ab6-81ef8a78d652",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (4, 240, 160) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(test_inputs)\n\u001b[1;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [post_trans(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decollate_batch(outputs)]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplot_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ground truth segmentation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plot_seg(outputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m95\u001b[39m)  \u001b[38;5;66;03m# Predicted segmentation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#plot_segmentation_results(test_inputs, test_labels, outputs, idx)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 32\u001b[0m, in \u001b[0;36mplot_seg\u001b[0;34m(image_seg, slice)\u001b[0m\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_seg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull Mask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(handles\u001b[38;5;241m=\u001b[39mlegend, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower left\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/monai_env/lib/python3.9/site-packages/matplotlib/pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m     sci(__ret)\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/monai_env/lib/python3.9/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/monai_env/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/monai_env/lib/python3.9/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/monai_env/lib/python3.9/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (4, 240, 160) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After each batch or at the end of testing\n",
    "for idx, test_data in enumerate(test_loader):\n",
    "    test_inputs = torch.cat([test_data['t1ce'].to(device), test_data['flair'].to(device)], dim=1)\n",
    "    test_labels = test_data['seg'].to(device)\n",
    "    outputs = model(test_inputs)\n",
    "    outputs = [post_trans(i) for i in decollate_batch(outputs)]\n",
    "    plot_seg(test_labels[0], 95)  # Ground truth segmentation\n",
    "    plot_seg(outputs[0], 95)  # Predicted segmentation\n",
    "    #plot_segmentation_results(test_inputs, test_labels, outputs, idx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf027ee-d06c-471c-8d0d-e235f3a9788d",
   "metadata": {},
   "source": [
    "### Compute precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69666428-ac13-489a-b30e-1a35fd87e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision and recall\n",
    "\n",
    "for idx, test_data in enumerate(test_loader):\n",
    "    test_inputs = torch.cat([test_data['t1ce'].to(device), test_data['flair'].to(device)], dim=1)\n",
    "    test_labels = test_data['seg'].to(device)\n",
    "    outputs = model(test_inputs)\n",
    "    outputs = [post_trans(i) for i in decollate_batch(outputs)]\n",
    "    print(len(outputs))\n",
    "    print(len(test_labels))\n",
    "    print(outputs.shape())\n",
    "    break\n",
    "    precision = compute_confusion_matrix_metric(outputs, test_labels, metric_name=\"precision\")\n",
    "    recall = compute_confusion_matrix_metric(outputs, test_labels, metric_name=\"recall\")\n",
    "\n",
    "    print(f\"Precision: {precision.mean().item():.4f}\")\n",
    "    print(f\"Recall: {recall.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff78e9-40e0-4bcf-bd56-bd93549ac39c",
   "metadata": {},
   "source": [
    "### Confusion Matrix for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919f812-d7a6-436e-952b-de6d5ae0c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have outputs and test_labels in the proper format (flattened, binary)\n",
    "y_pred = outputs.flatten().cpu().numpy()\n",
    "y_true = test_labels.flatten().cpu().numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
