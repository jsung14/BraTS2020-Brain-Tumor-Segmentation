{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72512b35",
   "metadata": {},
   "source": [
    "# BraTS2020 Brain Tumor Segmentation\n",
    "\n",
    "Takes data from https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation, which is from the BraTS2020 Competition. <br><br> There are 4 goals of the project:\n",
    "1. Manual segmentation labels of tumor sub-regions,\n",
    "2. Clinical data of overall survival,\n",
    "3. Clinical evaluation of progression status,\n",
    "4. Uncertainty estimation for the predicted tumor sub-regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d2839",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your working directory first so the data downloads where you want\n",
    "! kaggle datasets download awsaf49/brats20-dataset-training-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip brats20-dataset-training-validation.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbab6e",
   "metadata": {},
   "source": [
    "## 2. Load/Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import onnxruntime\n",
    "import random\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandRotated,\n",
    "    RandZoomd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a947b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"Your path\"\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc695e",
   "metadata": {},
   "source": [
    "The segmentation file in \"BraTS20_Training_355\" folder has an incorrect name. Before moving forward, rename it to maintain similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82156a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to 355 folder for renaming\n",
    "rename_PATH = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/\"\n",
    "print(rename_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_name = rename_PATH + \"W39_1998.09.19_Segm.nii\"\n",
    "new_name = rename_PATH + \"BraTS20_Training_355_seg.nii\"\n",
    "\n",
    "try:\n",
    "    os.rename(old_name, new_name)\n",
    "    print(\"File has been re-named successfully!\")\n",
    "except:\n",
    "    print(\"File is already renamed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08081a",
   "metadata": {},
   "source": [
    "Now that the files are named correctly, we can explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d878b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .nii file as a numpy array\n",
    "test_PATH = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "test_image_flair = nib.load(test_PATH + \"BraTS20_Training_355/BraTS20_Training_355_flair.nii\").get_fdata()\n",
    "print(\"Shape: \", test_image_flair.shape)\n",
    "print(\"Dtype: \", test_image_flair.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: \", test_image_flair.min())\n",
    "print(\"Max: \", test_image_flair.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5b264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbfd6a78",
   "metadata": {},
   "source": [
    "## 3. Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ae8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your training data directory\n",
    "train_data_dir = root_dir + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
    "\n",
    "# Get all patient directories in the training set\n",
    "train_dirs = sorted(glob.glob(os.path.join(train_data_dir, \"BraTS20_Training_*\")))\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary contains paths to the modalities and segmentation\n",
    "train_files = []\n",
    "for train_dir in train_dirs:\n",
    "    data_dict = {\n",
    "        \"t1\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_t1.nii.gz\"),\n",
    "        \"t1ce\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_t1ce.nii.gz\"),\n",
    "        \"t2\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_t2.nii.gz\"),\n",
    "        \"flair\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_flair.nii.gz\"),\n",
    "        \"seg\": os.path.join(train_dir, f\"{os.path.basename(train_dir)}_seg.nii.gz\"),\n",
    "    }\n",
    "    train_files.append(data_dict)\n",
    "\n",
    "# Print the number of training patients\n",
    "print(f\"Total number of training patients: {len(train_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae66e86",
   "metadata": {},
   "source": [
    "Since we don't have a defined test set, we can split the validation set randomly to get a train/val/test split to about 75/12.5/12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your validation data directory\n",
    "val_data_dir = root_dir + \"/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/\"\n",
    "\n",
    "# Get all patient directories in the validation set\n",
    "val_patient_dirs = sorted(glob.glob(os.path.join(val_data_dir, \"BraTS20_Validation_*\")))\n",
    "\n",
    "# Print the number of validation patients\n",
    "print(f\"Total number of validation and test patients: {len(val_patient_dirs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af341129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation into new validation and test sets (50/50 split)\n",
    "val_dirs, test_dirs = train_test_split(val_patient_dirs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create a list of dictionaries, where each dictionary contains paths to the modalities and segmentation\n",
    "val_files = []\n",
    "for val_dir in val_dirs:\n",
    "    data_dict = {\n",
    "        \"t1\": os.path.join(val_dir, f\"{os.path.basename(val_dir)}_t1.nii.gz\"),\n",
    "        \"t1ce\": os.path.join(val_dir, f\"{os.path.basename(val_dir)}_t1ce.nii.gz\"),\n",
    "        \"t2\": os.path.join(val_dir, f\"{os.path.basename(val_dir)}_t2.nii.gz\"),\n",
    "        \"flair\": os.path.join(val_dir, f\"{os.path.basename(val_dir)}_flair.nii.gz\"),\n",
    "    }\n",
    "    val_files.append(data_dict)\n",
    "\n",
    "    # Create a list of dictionaries, where each dictionary contains paths to the modalities and segmentation\n",
    "test_files = []\n",
    "for test_dir in test_dirs:\n",
    "    data_dict = {\n",
    "        \"t1\": os.path.join(test_dir, f\"{os.path.basename(test_dir)}_t1.nii.gz\"),\n",
    "        \"t1ce\": os.path.join(test_dir, f\"{os.path.basename(test_dir)}_t1ce.nii.gz\"),\n",
    "        \"t2\": os.path.join(test_dir, f\"{os.path.basename(test_dir)}_t2.nii.gz\"),\n",
    "        \"flair\": os.path.join(test_dir, f\"{os.path.basename(test_dir)}_flair.nii.gz\"),\n",
    "    }\n",
    "    test_files.append(data_dict)\n",
    "\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"New Validation set size: {len(val_files)}\")\n",
    "print(f\"Test set size: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c00e13",
   "metadata": {},
   "source": [
    "### Check the data before transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original train size: {len(train_files)}\")\n",
    "print(f\"Original val size: {len(val_files)}\")\n",
    "print(f\"Original test size: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dd970",
   "metadata": {},
   "source": [
    "## 5. Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    LoadImage, EnsureChannelFirst, Compose, ScaleIntensity, RandFlip, RandRotate, RandZoom\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "\n",
    "# Define transformations for training, validation, and test datasets\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\", \"seg\"]),  # Load images from file paths\n",
    "        EnsureChannelFirstd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\", \"seg\"]),  # Ensure channels are first\n",
    "        ScaleIntensityd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\"]),  # Normalize intensity values\n",
    "        RandFlipd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\", \"seg\"], spatial_axis=0, prob=0.5),  # Random flip\n",
    "        RandRotated(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\", \"seg\"], range_x=0.4, prob=0.5, keep_size=True),  # Random rotation\n",
    "        RandZoomd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\", \"seg\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),  # Random zoom\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\"]),\n",
    "        EnsureChannelFirstd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\"]),\n",
    "        ScaleIntensityd(keys=[\"t1\", \"t1ce\", \"t2\", \"flair\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MONAI datasets for training, new validation, and test sets\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "test_ds = Dataset(data=test_files, transform=val_transforms)  # Test set with no augmentation\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=8)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, num_workers=8)\n",
    "\n",
    "# Print data loader sizes\n",
    "print(f\"Training DataLoader size: {len(train_loader)}\")\n",
    "print(f\"Validation DataLoader size: {len(val_loader)}\")\n",
    "print(f\"Test DataLoader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdef90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
